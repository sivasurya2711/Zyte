import requests
import pandas as pd
from bs4 import BeautifulSoup
import urllib.parse as up
def Spider(webpagae_link):
    page = requests.get(webpagae_link)
    soup = BeautifulSoup(page.content, 'html.parser')
    books = soup.select("div ol")
    for i in books:
        image_url = i.find_all(class_="thumbnail")
        book_image_url = [base_url + j.get("src").replace("../../../", "") for j in image_url]
        title = i.select("h3 a")
        book_title = [k.get('title') for k in title]
        book_details_page_url = [base_url + k.get("href").replace("../../", 'catalogue/') for k in title]
        price = i.find_all(class_="price_color")
        for l in price:
            book_price = l.get_text()

    data = pd.DataFrame({
        "title_of_book": book_title,
        "price_of_book": book_price,
        "detail_page_URL_of_the_book": book_details_page_url,
        "image_url_of_book": book_image_url
    })
    df = pd.DataFrame(data,
                      columns=['title_of_book', 'price_of_book', 'detail_page_URL_of_the_book', 'image_url_of_book'])
    with open("/Users/surya/Desktop/zyte_assesment1.csv", "a") as temp:
        df.to_csv(temp, index=False, header=header_value)
    header_value  = False
    df = ""
    next_page_id = soup.find(class_="next")
    if (next_page_id):
        if(next_page_id.get_text()=="next"):
            for temp in next_page_id:
                link = temp.get("href")
                url = webpagae_link.rsplit("/",1)
                # print(url[0]+"/"+link)
                Spider(url[0]+"/"+link)

base_url = "http://books.toscrape.com/"
page = requests.get(base_url)
soup = BeautifulSoup(page.content, 'html.parser')
all_links = []
header_value = True
links = soup.find(class_="nav nav-list")
ll = links.find_all("a")
for i in ll:
    all_links.append(base_url+"/"+i.get('href'))
for webpagae_link in all_links[1:]:
    # print(webpagae_link)
    Spider(webpagae_link)
